// Service frontend pour Ollama
import { apiClient } from '../lib/api';

export interface OllamaGenerateRequest {
  prompt: string;
  model: string;
  options?: {
    temperature?: number;
    top_p?: number;
    max_tokens?: number;
  };
}

export interface OllamaWorkflowRequest {
  description: string;
  model: string;
  context?: any;
}

export interface OllamaModel {
  id?: string;
  name?: string;
  size?: number;
  modified_at?: string;
  created?: string;
  object?: string;
}

class OllamaService {
  private baseUrl = '/ollama';

  // Tester la connexion Ollama
  async testConnection(): Promise<{ success: boolean; models?: string[]; message?: string; error?: string }> {
    try {
      console.log('üîç [Ollama] Test de connexion...');
      const response = await apiClient.request(`${this.baseUrl}/test`);
      console.log('‚úÖ [Ollama] Connexion test√©e');
      return response;
    } catch (error) {
      console.error('‚ùå [Ollama] Erreur test connexion:', error);
      throw error;
    }
  }

  // Lister les mod√®les disponibles
  async getModels(): Promise<OllamaModel[]> {
    try {
      console.log('üìã [Ollama] R√©cup√©ration des mod√®les...');
      const response = await apiClient.request(`${this.baseUrl}/models`);
      console.log('‚úÖ [Ollama] Mod√®les r√©cup√©r√©s');
      return response.models || [];
    } catch (error) {
      console.error('‚ùå [Ollama] Erreur r√©cup√©ration mod√®les:', error);
      throw error;
    }
  }

  // R√©cup√©rer les mod√®les disponibles (alias pour compatibilit√©)
  async getAvailableModels(): Promise<Array<{id: string, name: string}>> {
    try {
      console.log('üìã [LocalAI] R√©cup√©ration des mod√®les disponibles...');
      const response = await apiClient.request(`${this.baseUrl}/models`);
      console.log('‚úÖ [LocalAI] Mod√®les r√©cup√©r√©s:', response.models?.length || 0);
      
      if (response.success && response.models) {
        return response.models.map((m: OllamaModel) => {
          const modelId = m.id || m.name || '';
          // Formater le nom pour un affichage plus lisible
          const formattedName = this.formatModelName(modelId);
          return {
            id: modelId,
            name: formattedName
          };
        });
      }
      return [];
    } catch (error) {
      console.error('‚ùå [LocalAI] Erreur r√©cup√©ration mod√®les:', error);
      throw error;
    }
  }

  // Formater le nom d'un mod√®le pour un affichage lisible
  private formatModelName(modelId: string): string {
    if (!modelId) return 'Mod√®le inconnu';
    
    // Remplacer les underscores et tirets par des espaces
    let formatted = modelId
      .replace(/_/g, ' ')
      .replace(/-/g, ' ')
      .replace(/([a-z])([A-Z])/g, '$1 $2'); // Ajouter espace avant majuscules
    
    // Mettre en forme selon les patterns connus
    if (modelId.includes('mistral')) {
      formatted = formatted.replace(/mistral\s*(\d+b?)/i, 'Mistral $1');
      if (modelId.includes('instruct')) {
        formatted += ' Instruct';
      }
      if (modelId.includes('v0.3')) {
        formatted += ' v0.3';
      }
    } else if (modelId.includes('gemma')) {
      formatted = formatted.replace(/gemma/i, 'Gemma');
      if (modelId.includes('27b')) {
        formatted = formatted.replace(/27\s*b/i, '27B');
      }
      if (modelId.includes('it')) {
        formatted += ' IT';
      }
    } else if (modelId.includes('qwen') || modelId.includes('Qwen')) {
      formatted = formatted.replace(/qwen/i, 'Qwen');
      if (modelId.includes('3')) {
        formatted += ' 3';
      }
      if (modelId.includes('coder')) {
        formatted += ' Coder';
      }
      if (modelId.includes('480B')) {
        formatted += ' 480B';
      }
    } else if (modelId.includes('planetoid')) {
      formatted = formatted.replace(/planetoid/i, 'Planetoid');
      if (modelId.includes('27b')) {
        formatted = formatted.replace(/27\s*b/i, '27B');
      }
      if (modelId.includes('v.2')) {
        formatted += ' v.2';
      }
    } else if (modelId.includes('openai') || modelId.includes('gpt')) {
      formatted = formatted.replace(/openai/i, 'OpenAI').replace(/gpt/i, 'GPT');
      if (modelId.includes('oss')) {
        formatted += ' OSS';
      }
      if (modelId.includes('20b')) {
        formatted += ' 20B';
      }
      if (modelId.includes('neo')) {
        formatted += ' Neo';
      }
    }
    
    // Capitaliser la premi√®re lettre
    return formatted.charAt(0).toUpperCase() + formatted.slice(1);
  }

  // G√©n√©rer du contenu
  async generate(request: OllamaGenerateRequest): Promise<{
    content: string;
    metadata: {
      model: string;
      generationTime: number;
      tokens: number;
    };
  }> {
    try {
      console.log('ü§ñ [Ollama] G√©n√©ration de contenu...');
      const response = await apiClient.request(`${this.baseUrl}/generate`, {
        method: 'POST',
        body: JSON.stringify(request)
      });
      console.log('‚úÖ [Ollama] Contenu g√©n√©r√©');
      return response;
    } catch (error) {
      console.error('‚ùå [Ollama] Erreur g√©n√©ration:', error);
      throw error;
    }
  }

  // G√©n√©rer un workflow
  async generateWorkflow(request: OllamaWorkflowRequest): Promise<{
    workflow: any;
    metadata: {
      model: string;
      generationTime: number;
      tokens: number;
    };
  }> {
    try {
      console.log('üîß [Ollama] G√©n√©ration de workflow...');
      const response = await apiClient.request(`${this.baseUrl}/generate-workflow`, {
        method: 'POST',
        body: JSON.stringify(request)
      });
      console.log('‚úÖ [Ollama] Workflow g√©n√©r√©');
      return response;
    } catch (error) {
      console.error('‚ùå [Ollama] Erreur g√©n√©ration workflow:', error);
      throw error;
    }
  }
}

export const ollamaService = new OllamaService();

# üîß Configuration Ollama dans Docker

## üìã Situation

Ollama est install√© dans un conteneur Docker sur votre VPS (`147.93.58.155`).

## üéØ Configuration

### Option 1 : Backend aussi dans Docker (m√™me r√©seau)

Si votre backend est aussi dans Docker et dans le m√™me r√©seau que Ollama :

**Variables d'environnement** (`backend/.env`) :
```bash
OLLAMA_URL=http://localai:19080
```

O√π `localai` est le nom de votre conteneur Ollama.

### Option 2 : Backend hors Docker

Si votre backend n'est pas dans Docker, utilisez l'IP du VPS + port mapp√© :

**Variables d'environnement** (`backend/.env`) :
```bash
OLLAMA_URL=http://147.93.58.155:19080
```

## üîç V√©rifications n√©cessaires

### 1. Nom du conteneur Ollama

```bash
# Sur le VPS
docker ps | grep localai
```

Notez le nom du conteneur (colonne NAME). Le conteneur devrait s'appeler `localai`.

### 2. Port mapp√©

```bash
# Sur le VPS
docker ps | grep localai
# Chercher dans la colonne PORTS (ex: 0.0.0.0:19080->19080/tcp)
```

Si le port est mapp√© `19080:19080`, utilisez l'IP VPS + port.

### 3. R√©seau Docker

```bash
# Voir les r√©seaux
docker network ls

# Voir les conteneurs dans le r√©seau
docker network inspect <network_name>
```

### 4. Test de connexion

Depuis le backend (ou VPS) :
```bash
# Si backend dans Docker avec m√™me r√©seau
curl http://localai:19080/api/tags

# Si backend hors Docker
curl http://147.93.58.155:19080/api/tags
```

## üöÄ Configuration recommand√©e

### Si backend dans Docker

1. **Cr√©er ou utiliser un r√©seau Docker partag√©** :
```bash
docker network create automivy-network
```

2. **Connecter Ollama au r√©seau** :
```bash
docker network connect automivy-network localai
```

3. **Connecter le backend au r√©seau** :
```bash
docker network connect automivy-network automivy-backend
```

4. **Configuration** (`backend/.env`) :
```bash
OLLAMA_URL=http://localai:19080
```

### Si backend hors Docker

1. **V√©rifier que le port est mapp√©** :
```bash
docker ps | grep localai
# Doit afficher : 0.0.0.0:19080->19080/tcp
```

2. **Configuration** (`backend/.env`) :
```bash
OLLAMA_URL=http://147.93.58.155:19080
```

## ‚úÖ Test de connexion

Apr√®s configuration, tester depuis le backend :

```bash
curl http://147.93.58.155:19080/api/tags
# OU
curl http://localai:19080/api/tags
```

Si cela fonctionne, vous devriez voir une liste de mod√®les JSON.

## üîß D√©pannage

### Erreur : Connection refused

**Probl√®me** : Ollama n'est pas accessible depuis le backend

**Solutions** :
1. V√©rifier que le conteneur Ollama est lanc√© : `docker ps | grep localai`
2. V√©rifier le port mapp√© : `docker ps | grep localai`
3. V√©rifier le firewall : `ufw status` (doit permettre le port 19080)

### Erreur : Network unreachable

**Probl√®me** : Backend et Ollama dans des r√©seaux Docker diff√©rents

**Solutions** :
1. Cr√©er un r√©seau partag√© : `docker network create automivy-network`
2. Connecter les deux conteneurs au r√©seau
3. Utiliser le nom du conteneur dans l'URL

### Erreur : Timeout

**Probl√®me** : Ollama est lent ou surcharg√©

**Solutions** :
1. V√©rifier les ressources : `docker stats localai`
2. Utiliser un mod√®le plus petit : `phi3:mini` au lieu de `llama3.1:8b`
3. Augmenter le timeout dans le code


# üöÄ Configuration Ollama (Local AI) - Conteneur Docker

## üìã Configuration actuelle

**Conteneur Ollama** : `localai`  
**VPS** : `147.93.58.155`  
**Port** : `19080`

## üéØ Configuration selon votre setup

### Option 1 : Backend AUSSI dans Docker (m√™me r√©seau) ‚úÖ RECOMMAND√â

Si votre backend tourne aussi dans Docker et dans le m√™me r√©seau que `localai` :

**1. Cr√©er ou v√©rifier le r√©seau Docker partag√©** :
```bash
# Sur le VPS
docker network create automivy-network
# OU utiliser un r√©seau existant (ex: proxy, bridge, etc.)
```

**2. Connecter les conteneurs au m√™me r√©seau** :
```bash
# Connecter localai au r√©seau
docker network connect automivy-network localai

# Connecter le backend au r√©seau
docker network connect automivy-network automivy-backend
# OU le nom de votre conteneur backend
```

**3. Configuration** (`backend/.env`) :
```bash
OLLAMA_URL=http://localai:19080
```

**4. Red√©marrer le backend** pour que les changements prennent effet.

### Option 2 : Backend HORS Docker

Si votre backend tourne directement sur le VPS (pas dans Docker) :

**1. V√©rifier que le port est mapp√©** :
```bash
# Sur le VPS
docker ps | grep localai
# Cherchez : 0.0.0.0:11434->11434/tcp dans la colonne PORTS
```

**2. Configuration** (`backend/.env`) :
```bash
OLLAMA_URL=http://147.93.58.155:19080
# OU si backend sur localhost
OLLAMA_URL=http://localhost:19080
```

**3. Red√©marrer le backend** pour que les changements prennent effet.

## ‚úÖ Test de connexion

### Depuis le VPS :

```bash
# Si backend dans Docker (m√™me r√©seau)
curl http://localai:19080/api/tags

# Si backend hors Docker
curl http://147.93.58.155:19080/api/tags
# OU
curl http://localhost:19080/api/tags
```

Si cela fonctionne, vous devriez voir une liste JSON de mod√®les Ollama.

## üîß √âtapes de configuration

### 1. Cr√©er/Modifier `backend/.env`

```bash
# Dans le r√©pertoire backend/
nano .env
# OU
vi .env
```

Ajouter :
```bash
OLLAMA_URL=http://localai:11434
# OU
OLLAMA_URL=http://147.93.58.155:11434
```

### 2. Red√©marrer le backend

```bash
# Si backend dans Docker
docker restart automivy-backend

# Si backend hors Docker
# Arr√™ter avec Ctrl+C puis relancer
npm start
```

### 3. V√©rifier les logs

Regardez les logs du backend pour voir :
```
üîß [Ollama] URL configur√©e: http://localai:19080
```

### 4. Tester depuis l'interface

1. Aller dans Admin ‚Üí AI Generator
2. S√©lectionner "Local AI (Ollama - Gratuit)"
3. Choisir un mod√®le (ex: Llama 3.1 8B)
4. Entrer une description de workflow
5. Cliquer sur "Generate with AI"

## üîç D√©pannage

### Erreur : "Cannot connect to localai:11434"

**Probl√®me** : Backend et Ollama pas dans le m√™me r√©seau Docker

**Solution** :
1. V√©rifier les r√©seaux : `docker network ls`
2. Connecter les deux conteneurs au m√™me r√©seau
3. Utiliser l'IP VPS si backend hors Docker : `OLLAMA_URL=http://147.93.58.155:11434`

### Erreur : "Connection refused"

**Probl√®me** : Port non mapp√© ou Ollama non accessible

**Solution** :
1. V√©rifier que `localai` tourne : `docker ps | grep localai`
2. V√©rifier le port mapp√© : `docker ps | grep localai` (colonne PORTS)
3. Utiliser l'IP VPS : `OLLAMA_URL=http://147.93.58.155:19080`

### Erreur : "Network unreachable"

**Probl√®me** : R√©seau Docker incorrect

**Solution** :
1. V√©rifier le r√©seau : `docker network inspect <network_name>`
2. Cr√©er un r√©seau partag√© : `docker network create automivy-network`
3. Connecter les conteneurs : `docker network connect automivy-network localai`

## üìù R√©sum√©

**Conteneur** : `localai`  
**URL Docker** : `http://localai:19080`  
**URL VPS** : `http://147.93.58.155:19080`

**Configuration** (`backend/.env`) :
- Si backend dans Docker : `OLLAMA_URL=http://localai:19080`
- Si backend hors Docker : `OLLAMA_URL=http://147.93.58.155:19080`

**Action requise** : Cr√©er/modifier `backend/.env` et red√©marrer le backend.


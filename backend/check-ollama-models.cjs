// Script pour vÃ©rifier les modÃ¨les installÃ©s sur Ollama (localai)
const fetch = require('node-fetch');
const path = require('path');
require('dotenv').config({ path: path.join(__dirname, '.env') });
require('dotenv').config({ path: path.join(__dirname, '..', '.env') });

async function checkOllamaModels() {
  // Essayer plusieurs URLs possibles
  // Port mappÃ© Docker: 19080 (hÃ´te) -> 8080 (conteneur)
  const urls = [
    process.env.OLLAMA_URL,
    process.env.VITE_OLLAMA_URL,
    'http://147.93.58.155:19080', // Port mappÃ© sur l'hÃ´te (dev local)
    'http://localai:8080', // Port interne du conteneur (production Docker)
    'http://147.93.58.155:8080', // Essai avec port interne
    'http://localhost:19080',
    'http://127.0.0.1:19080'
  ].filter(Boolean); // Filtrer les valeurs nulles/undefined

  console.log('ğŸ” VÃ©rification des modÃ¨les LocalAI installÃ©s sur localai...\n');
  console.log('ğŸ“‹ URLs Ã  tester:');
  urls.forEach((url, index) => {
    console.log(`   ${index + 1}. ${url}`);
  });
  console.log('');

  for (const baseUrl of urls) {
    try {
      console.log(`ğŸŒ Test de connexion Ã : ${baseUrl}`);
      
      // LocalAI utilise /v1/models (format OpenAI)
      const modelsUrl = `${baseUrl}/v1/models`;
      console.log(`   URL complÃ¨te: ${modelsUrl}`);
      
      const response = await fetch(modelsUrl, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        },
        timeout: 5000
      });

      if (!response.ok) {
        const text = await response.text();
        console.log(`   âŒ Erreur HTTP ${response.status}: ${response.statusText}`);
        console.log(`   RÃ©ponse: ${text.substring(0, 200)}`);
        continue;
      }

      const contentType = response.headers.get('content-type');
      if (!contentType || !contentType.includes('application/json')) {
        const text = await response.text();
        console.log(`   âš ï¸  RÃ©ponse non-JSON: ${contentType}`);
        console.log(`   RÃ©ponse: ${text.substring(0, 200)}`);
        continue;
      }

      const data = await response.json();
      
      console.log(`   âœ… Connexion rÃ©ussie!`);
      // Format OpenAI: { data: [{ id: "...", ... }] }
      const models = data.data || data.models || [];
      console.log(`   ğŸ“¦ Nombre de modÃ¨les trouvÃ©s: ${models.length}\n`);

      if (models.length > 0) {
        console.log('ğŸ“‹ ModÃ¨les installÃ©s:');
        console.log('â”€'.repeat(60));
        models.forEach((model, index) => {
          const modelId = model.id || model.name || 'N/A';
          console.log(`\n${index + 1}. ModÃ¨le: ${modelId}`);
          if (model.size) {
            console.log(`   - Taille: ${(model.size / 1024 / 1024 / 1024).toFixed(2)} GB`);
          }
          if (model.digest) {
            console.log(`   - Digest: ${model.digest.substring(0, 12)}...`);
          }
          if (model.created) {
            console.log(`   - CrÃ©Ã©: ${model.created}`);
          }
          if (model.object) {
            console.log(`   - Type: ${model.object}`);
          }
        });
        console.log('\n' + 'â”€'.repeat(60));
        
        // ModÃ¨les compatibles avec l'AI Generator
        const compatibleModels = models.map(m => m.id || m.name).filter(name => {
          return name && (name.includes('phi3') || 
                 name.includes('llama') || 
                 name.includes('mistral') || 
                 name.includes('codellama') ||
                 name.includes('gpt') ||
                 name.includes('claude'));
        });

        if (compatibleModels.length > 0) {
          console.log('\nâœ… ModÃ¨les compatibles avec l\'AI Generator:');
          compatibleModels.forEach(model => console.log(`   - ${model}`));
        } else {
          console.log('\nâš ï¸  Aucun modÃ¨le compatible trouvÃ©.');
          console.log('   ModÃ¨les recommandÃ©s:');
          console.log('   - phi3:mini');
          console.log('   - phi3:3.8b');
          console.log('   - llama3.1:8b');
          console.log('   - mistral:7b');
          console.log('   - codellama:latest');
        }

        console.log(`\nâœ… URL fonctionnelle: ${baseUrl}`);
        console.log(`\nğŸ”§ Configuration recommandÃ©e dans backend/.env:`);
        console.log(`   OLLAMA_URL=${baseUrl}`);
        
        return; // ArrÃªter aprÃ¨s le premier succÃ¨s
      } else {
        console.log(`   âš ï¸  Aucun modÃ¨le trouvÃ© sur ${baseUrl}`);
      }
    } catch (error) {
      console.log(`   âŒ Erreur: ${error.message}`);
      if (error.code === 'ENOTFOUND') {
        console.log(`      â†’ Le nom d'hÃ´te "${baseUrl.split('://')[1].split(':')[0]}" ne peut pas Ãªtre rÃ©solu`);
        console.log(`      â†’ Utiliser l'IP VPS (147.93.58.155) si le backend n'est pas dans Docker`);
      } else if (error.code === 'ECONNREFUSED') {
        console.log(`      â†’ Connexion refusÃ©e - vÃ©rifier que Ollama est dÃ©marrÃ©`);
      } else if (error.code === 'ETIMEDOUT') {
        console.log(`      â†’ Timeout - vÃ©rifier la connectivitÃ© rÃ©seau`);
      }
      console.log('');
    }
  }

  console.log('\nâŒ Aucune connexion rÃ©ussie.');
  console.log('\nğŸ” DÃ©pannage:');
  console.log('1. VÃ©rifier que le conteneur localai est dÃ©marrÃ©: docker ps | grep localai');
  console.log('2. VÃ©rifier le port mappÃ©: docker ps | grep localai (colonne PORTS)');
  console.log('   â†’ Port mappÃ© attendu: 19080:8080 (hÃ´te:conteneur)');
  console.log('3. Si backend en dev (PC local), utiliser: OLLAMA_URL=http://147.93.58.155:19080');
  console.log('4. Si backend dans Docker, utiliser: OLLAMA_URL=http://localai:8080');
}

checkOllamaModels().catch(error => {
  console.error('âŒ Erreur fatale:', error);
  process.exit(1);
});


const express = require('express');
const router = express.Router();
const { authenticateToken } = require('../middleware/auth');
const ollamaService = require('../services/ollamaService');

// Tester la connexion LocalAI
router.get('/test', authenticateToken, async (req, res) => {
  try {
    const result = await ollamaService.testConnection();
    res.json(result);
  } catch (error) {
    console.error('‚ùå [LocalAI] Erreur test:', error);
    res.status(500).json({ 
      success: false, 
      error: 'Erreur lors du test de connexion LocalAI' 
    });
  }
});

// Lister les mod√®les disponibles
router.get('/models', authenticateToken, async (req, res) => {
  try {
    console.log('üìã [LocalAI] R√©cup√©ration des mod√®les disponibles...');
    const models = await ollamaService.getAvailableModels();
    console.log(`‚úÖ [LocalAI] ${models.length} mod√®les trouv√©s`);
    
    // Formater les mod√®les pour le frontend
    const formattedModels = models.map(m => ({
      id: m.id || m.name,
      name: m.name || m.id,
      size: m.size,
      modified_at: m.modified_at || m.created,
      object: m.object || 'model'
    }));
    
    res.json({ 
      success: true, 
      models: formattedModels,
      count: formattedModels.length
    });
  } catch (error) {
    console.error('‚ùå [LocalAI] Erreur mod√®les:', error);
    console.error('‚ùå [LocalAI] Stack:', error.stack);
    res.status(500).json({ 
      success: false, 
      error: 'Erreur lors de la r√©cup√©ration des mod√®les',
      message: error.message 
    });
  }
});

// G√©n√©rer un workflow avec LocalAI
router.post('/generate-workflow', authenticateToken, async (req, res) => {
  try {
    const { description, model, context } = req.body;
    
    if (!description) {
      return res.status(400).json({ 
        success: false, 
        error: 'Description requise' 
      });
    }

    // Utiliser le mod√®le demand√© ou null pour utiliser le d√©faut du service
    const modelToUse = model || null;

    const result = await ollamaService.generateWorkflow(
      description, 
      modelToUse,
      context || {}
    );

    res.json({
      success: true,
      workflow: result.workflow,
      metadata: result.metadata
    });
  } catch (error) {
    console.error('‚ùå [LocalAI] Erreur g√©n√©ration workflow:', error);
    console.error('‚ùå [LocalAI] Stack:', error.stack);
    console.error('‚ùå [LocalAI] Message:', error.message);
    
    // Message d'erreur plus utile
    let errorMessage = error.message || 'Erreur lors de la g√©n√©ration du workflow';
    if (errorMessage.includes('could not load model')) {
      errorMessage = 'Le mod√®le demand√© n\'est pas disponible ou ne peut pas √™tre charg√© sur LocalAI. V√©rifiez les mod√®les install√©s.';
    }
    
    res.status(500).json({ 
      success: false, 
      error: errorMessage,
      details: process.env.NODE_ENV === 'development' ? error.stack : undefined
    });
  }
});

// G√©n√©rer du contenu libre
router.post('/generate', authenticateToken, async (req, res) => {
  try {
    const { prompt, model, options } = req.body;
    
    if (!prompt) {
      return res.status(400).json({ 
        success: false, 
        error: 'Prompt requis' 
      });
    }

    const result = await ollamaService.generateContent(
      prompt, 
      model || 'llama3.1:8b',
      options || {}
    );

    res.json({
      success: true,
      content: result.content,
      metadata: {
        model: result.model,
        generationTime: result.total_duration,
        tokens: result.eval_count
      }
    });
  } catch (error) {
    console.error('‚ùå [LocalAI] Erreur g√©n√©ration:', error);
    res.status(500).json({ 
      success: false, 
      error: 'Erreur lors de la g√©n√©ration' 
    });
  }
});

module.exports = router;

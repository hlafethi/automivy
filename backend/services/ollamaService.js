// Service pour int√©grer LocalAI avec l'AI Generator
// LocalAI utilise une API compatible OpenAI, diff√©rente d'Ollama
const fetch = require('node-fetch');

class OllamaService {
  constructor() {
    // Configuration LocalAI (pas Ollama)
    // Support Docker : si backend et LocalAI dans Docker, utiliser nom conteneur (localai:8080)
    // Support dev local : utiliser IP VPS + port mapp√© (147.93.58.155:19080)
    // Port mapp√© Docker: 19080 (h√¥te) -> 8080 (conteneur)
    // Par d√©faut en dev: utiliser l'IP VPS avec le port mapp√©
    this.baseUrl = process.env.OLLAMA_URL || process.env.VITE_OLLAMA_URL || 'http://147.93.58.155:19080';
    console.log(`üîß [LocalAI] URL configur√©e: ${this.baseUrl}`);
    // Mod√®les disponibles sur LocalAI (sera mis √† jour dynamiquement)
    this.availableModels = [
      'qwen2.5-72b-instruct',  // Mod√®le Qwen 3 recommand√© par d√©faut
      'qwen2.5-72b',  // Variante du nom
      'qwen-2.5-72b-instruct',  // Variante avec tiret
      'mistral-7b-instruct-v0.3',
      'gemma-3-27b-it',
      'openai_gpt-oss-20b-neo',
      'planetoid_27b_v.2',
      'llama3.1:8b',
      'mistral:7b',
      'phi3:mini'
    ];
    this.defaultModel = 'mistral-7b-instruct-v0.3'; // Mod√®le plus l√©ger par d√©faut (plus rapide)
  }

  // V√©rifier si LocalAI est disponible
  async isAvailable() {
    try {
      // LocalAI utilise /v1/models (format OpenAI)
      const response = await fetch(`${this.baseUrl}/v1/models`);
      return response.ok;
    } catch (error) {
      console.error('‚ùå [LocalAI] Service non disponible:', error.message);
      return false;
    }
  }

  // Lister les mod√®les disponibles
  async getAvailableModels() {
    try {
      console.log(`üìã [LocalAI] R√©cup√©ration des mod√®les depuis ${this.baseUrl}/v1/models`);
      // LocalAI utilise /v1/models (format OpenAI)
      const response = await fetch(`${this.baseUrl}/v1/models`);
      
      if (!response.ok) {
        const errorText = await response.text().catch(() => '');
        console.error(`‚ùå [LocalAI] Erreur HTTP ${response.status}:`, errorText);
        throw new Error(`LocalAI API error: ${response.status} ${response.statusText}`);
      }
      
      const data = await response.json();
      // Format OpenAI: { data: [{ id: "...", ... }] }
      const models = data.data || [];
      console.log(`‚úÖ [LocalAI] ${models.length} mod√®les trouv√©s:`, models.map(m => m.id || m.name).join(', '));
      return models;
    } catch (error) {
      console.error('‚ùå [LocalAI] Erreur lors de la r√©cup√©ration des mod√®les:', error.message);
      console.error('‚ùå [LocalAI] Stack:', error.stack);
      throw error; // Propager l'erreur pour que la route puisse la g√©rer
    }
  }

  // G√©n√©rer du contenu avec LocalAI (API compatible OpenAI)
  async generateContent(prompt, model = 'llama3.1:8b', options = {}) {
    try {
      console.log(`ü§ñ [LocalAI] G√©n√©ration avec ${model}:`, prompt.substring(0, 100) + '...');
      console.log(`ü§ñ [LocalAI] URL compl√®te: ${this.baseUrl}/v1/chat/completions`);
      
      // LocalAI utilise le format OpenAI avec messages (system + user)
      const messages = [];
      if (options.systemMessage) {
        messages.push({ role: 'system', content: options.systemMessage });
      }
      if (options.userMessage) {
        messages.push({ role: 'user', content: options.userMessage });
      } else {
        messages.push({ role: 'user', content: prompt });
      }
      
      // V√©rifier d'abord si le mod√®le est disponible en le testant
      try {
        console.log(`üîç [LocalAI] V√©rification du mod√®le ${model}...`);
        const modelsCheck = await this.getAvailableModels();
        const modelExists = modelsCheck.some(m => {
          const modelId = (m.id || m.name || '').toLowerCase();
          const requestedModel = model.toLowerCase();
          return modelId === requestedModel || modelId.includes(requestedModel) || requestedModel.includes(modelId.split(':')[0]);
        });
        
        if (!modelExists && modelsCheck.length > 0) {
          console.warn(`‚ö†Ô∏è [LocalAI] Mod√®le ${model} non trouv√© dans la liste, mod√®les disponibles:`, modelsCheck.map(m => m.id || m.name));
          // Essayer avec l'ID exact tel qu'il appara√Æt dans la liste
          const exactMatch = modelsCheck.find(m => {
            const modelId = (m.id || m.name || '').toLowerCase();
            return modelId.includes(model.toLowerCase().split('-')[0]) || model.toLowerCase().includes(modelId.split('-')[0]);
          });
          if (exactMatch) {
            const exactModelId = exactMatch.id || exactMatch.name;
            console.log(`üîÑ [LocalAI] Utilisation de l'ID exact du mod√®le: ${exactModelId}`);
            model = exactModelId;
          }
        }
      } catch (checkError) {
        console.warn(`‚ö†Ô∏è [LocalAI] Impossible de v√©rifier les mod√®les: ${checkError.message}`);
      }
      
      const requestBody = {
        model: model,
        messages: messages,
        stream: false,
        temperature: options.temperature || 0.7,
        max_tokens: options.max_tokens || 2000
      };
      
      console.log(`ü§ñ [LocalAI] Corps de la requ√™te:`, JSON.stringify(requestBody, null, 2));
      console.log(`üìù [LocalAI] Nom du mod√®le utilis√©: "${model}"`);
      console.log(`üìè [LocalAI] Taille du prompt: ${JSON.stringify(requestBody).length} caract√®res`);
      console.log(`üìè [LocalAI] Nombre de messages: ${requestBody.messages.length}`);
      console.log(`üìè [LocalAI] Taille totale des messages: ${requestBody.messages.reduce((sum, m) => sum + m.content.length, 0)} caract√®res`);
      
      // V√©rifier d'abord si LocalAI r√©pond rapidement
      console.log(`üîç [LocalAI] V√©rification de la disponibilit√© de LocalAI √† ${this.baseUrl}...`);
      const healthCheckStart = Date.now();
      try {
        // Essayer plusieurs endpoints possibles
        const healthEndpoints = ['/health', '/ready', '/v1/models'];
        let healthCheckOk = false;
        
        for (const endpoint of healthEndpoints) {
          try {
            const healthResponse = await fetch(`${this.baseUrl}${endpoint}`, {
              method: 'GET',
              signal: AbortSignal.timeout(5000) // 5 secondes max pour le health check
            });
            if (healthResponse.ok || healthResponse.status === 200) {
              const healthCheckTime = Date.now() - healthCheckStart;
              console.log(`‚úÖ [LocalAI] Health check OK via ${endpoint} en ${healthCheckTime}ms`);
              healthCheckOk = true;
              break;
            }
          } catch (e) {
            // Essayer le prochain endpoint
            continue;
          }
        }
        
        if (!healthCheckOk) {
          console.warn(`‚ö†Ô∏è [LocalAI] Aucun endpoint de health check disponible, mais on continue quand m√™me`);
        }
      } catch (healthError) {
        console.warn(`‚ö†Ô∏è [LocalAI] Health check √©chou√©: ${healthError.message}`);
        console.warn(`‚ö†Ô∏è [LocalAI] Le serveur LocalAI √† ${this.baseUrl} ne r√©pond peut-√™tre pas. V√©rifiez qu'il est d√©marr√© et accessible.`);
        // Continuer quand m√™me, certains serveurs n'ont pas de health check
      }
      
      // Timeout de 5 minutes (300 secondes) - les mod√®les locaux peuvent √™tre lents mais g√©n√®rent bien
      const timeoutMs = 300000; // 5 minutes
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), timeoutMs);
      
      const requestStartTime = Date.now();
      console.log(`üöÄ [LocalAI] Envoi de la requ√™te √† ${this.baseUrl}/v1/chat/completions...`);
      
      try {
        const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(requestBody),
          signal: controller.signal
        });
        
        const requestTime = Date.now() - requestStartTime;
        console.log(`‚è±Ô∏è [LocalAI] R√©ponse re√ßue en ${requestTime}ms`);
        clearTimeout(timeoutId);

        if (!response.ok) {
          const errorText = await response.text().catch(() => '');
          console.error(`‚ùå [LocalAI] Erreur HTTP ${response.status}:`, errorText);
          throw new Error(`LocalAI API error: ${response.status} ${response.statusText} - ${errorText.substring(0, 200)}`);
        }

        const data = await response.json();
        console.log('‚úÖ [LocalAI] G√©n√©ration termin√©e');
        
        // Format OpenAI: { choices: [{ message: { content: "..." } }] }
        if (!data.choices || !data.choices[0] || !data.choices[0].message) {
          console.error('‚ùå [LocalAI] R√©ponse invalide - format OpenAI attendu:', JSON.stringify(data, null, 2));
          throw new Error('LocalAI a retourn√© une r√©ponse invalide (format OpenAI attendu)');
        }
        
        return {
          content: data.choices[0].message.content,
          model: data.model || model,
          usage: data.usage || {}
        };
      } catch (error) {
        clearTimeout(timeoutId);
        const requestTime = Date.now() - requestStartTime;
        if (error.name === 'AbortError') {
          console.error(`‚è±Ô∏è [LocalAI] Timeout apr√®s ${timeoutMs/1000} secondes (${requestTime}ms √©coul√©s) avec le mod√®le ${model}`);
          console.error(`üîç [LocalAI] Diagnostic: Le serveur LocalAI √† ${this.baseUrl} ne r√©pond pas dans les temps.`);
          console.error(`üí° [LocalAI] Suggestions:`);
          console.error(`   1. V√©rifier que LocalAI est bien d√©marr√© et accessible`);
          console.error(`   2. V√©rifier que le mod√®le ${model} est charg√© en m√©moire`);
          console.error(`   3. V√©rifier les ressources serveur (CPU/RAM)`);
          console.error(`   4. Essayer un mod√®le plus l√©ger (mistral-7b-instruct-v0.3, gemma-3-27b-it)`);
          console.error(`   5. Utiliser OpenRouter au lieu de LocalAI pour une g√©n√©ration plus rapide`);
          throw new Error(`La g√©n√©ration a pris trop de temps (plus de ${timeoutMs/1000} secondes). Le serveur LocalAI √† ${this.baseUrl} ne r√©pond pas dans les temps. V√©rifiez que LocalAI est d√©marr√© et accessible, et que le mod√®le ${model} est charg√©. Essayez avec un mod√®le plus l√©ger ou utilisez OpenRouter pour une g√©n√©ration plus rapide.`);
        }
        throw error;
      }
    } catch (error) {
      console.error('‚ùå [LocalAI] Erreur lors de la g√©n√©ration:', error.message);
      console.error('‚ùå [LocalAI] Stack:', error.stack);
      console.error('‚ùå [LocalAI] URL LocalAI:', this.baseUrl);
      throw error;
    }
  }

  // G√©n√©rer un workflow n8n avec LocalAI
  async generateWorkflow(description, model = null, context = {}) {
    try {
      // Utiliser le mod√®le par d√©faut si aucun mod√®le n'est sp√©cifi√©
      if (!model) {
        model = this.defaultModel;
      }
      
      console.log(`üîß [LocalAI] D√©but g√©n√©ration workflow avec ${model}`);
      console.log(`üîß [LocalAI] URL LocalAI: ${this.baseUrl}`);
      
      // V√©rifier d'abord les mod√®les disponibles
      let availableModelIds = [];
      try {
        const availableModels = await this.getAvailableModels();
        console.log(`üìã [LocalAI] Mod√®les disponibles: ${availableModels.length}`);
        if (availableModels.length > 0) {
          availableModelIds = availableModels.map(m => m.id || m.name).filter(Boolean);
          console.log(`üìã [LocalAI] Mod√®les: ${availableModelIds.join(', ')}`);
          
          // V√©rifier si le mod√®le demand√© existe et utiliser l'ID exact de LocalAI
          const exactModel = availableModelIds.find(m => {
            const modelId = m.toLowerCase();
            const requestedModel = model.toLowerCase();
            // Correspondance exacte ou partielle
            return modelId === requestedModel || 
                   modelId.includes(requestedModel) || 
                   requestedModel.includes(modelId.split('-')[0]) ||
                   modelId.includes(requestedModel.split('-')[0]);
          });
          
          if (exactModel) {
            // Utiliser l'ID exact tel qu'il appara√Æt dans LocalAI
            console.log(`‚úÖ [LocalAI] Mod√®le trouv√©: ${exactModel} (demand√©: ${model})`);
            model = exactModel; // Utiliser l'ID exact
          } else {
            // Utiliser un mod√®le disponible (priorit√© aux mod√®les instruct)
            const fallbackModel = availableModelIds.find(m => 
              m.includes('instruct') || m.includes('mistral') || m.includes('gemma')
            ) || availableModelIds[0];
            
            if (fallbackModel) {
              console.log(`‚ö†Ô∏è [LocalAI] Mod√®le ${model} non trouv√©, utilisation de ${fallbackModel}`);
              model = fallbackModel;
            } else {
              throw new Error(`Aucun mod√®le disponible sur LocalAI`);
            }
          }
        } else {
          throw new Error(`Aucun mod√®le disponible sur LocalAI`);
        }
      } catch (modelError) {
        console.warn(`‚ö†Ô∏è [LocalAI] Impossible de r√©cup√©rer les mod√®les disponibles: ${modelError.message}`);
        // Continuer avec le mod√®le demand√© ou utiliser le d√©faut
        if (!model || model === this.defaultModel) {
          model = this.defaultModel;
        }
        // Si on n'a pas de liste de mod√®les, cr√©er une liste avec les mod√®les connus
        if (availableModelIds.length === 0) {
          availableModelIds = ['mistral-7b-instruct-v0.3', 'gemma-3-27b-it', 'openai_gpt-oss-20b-neo', 'planetoid_27b_v.2'];
        }
      }
      
      // Construire la liste des n≈ìuds populaires depuis le contexte
      const popularNodesList = context.popularNodes ? Object.keys(context.popularNodes).slice(0, 10).join(', ') : '';
      const templateExamples = context.templates ? context.templates.slice(0, 2).map(t => t.name).join(', ') : '';
      
      const systemPrompt = `Tu es un expert en g√©n√©ration de workflows n8n FONCTIONNELS. 
G√©n√®re un workflow JSON VALIDE et FONCTIONNEL bas√© sur cette description: "${description}"

CONTR√îLES QUALIT√â CRITIQUES:
1. Utilise UNIQUEMENT des types de n≈ìuds n8n VALIDES et TEST√âS
2. Chaque n≈ìud DOIT avoir des param√®tres r√©alistes et fonctionnels
3. Les connexions DOIVENT √™tre logiques et compl√®tes
4. Les credentials DOIVENT utiliser les placeholders dynamiques
5. Le JSON DOIT √™tre valide et parsable

NOEUDS N8N VALIDES UNIQUEMENT - Utilise SEULEMENT ces types:

TRIGGERS (un seul par workflow):
- "n8n-nodes-base.webhook" (pour d√©clencheurs HTTP)
- "n8n-nodes-base.schedule" (pour d√©clencheurs programm√©s)
- "n8n-nodes-base.manualTrigger" (pour d√©clencheurs manuels)

EMAIL:
- "n8n-nodes-base.emailReadImap" (lecture IMAP)
- "n8n-nodes-base.emailSend" (envoi SMTP)
- "n8n-nodes-imap.imap" (op√©rations IMAP)

COMMUNICATION:
- "n8n-nodes-base.slack" (Slack)
- "n8n-nodes-base.discord" (Discord)
- "n8n-nodes-base.telegram" (Telegram)

APIS & DONN√âES:
- "n8n-nodes-base.httpRequest" (requ√™tes HTTP)
- "n8n-nodes-base.postgres" (PostgreSQL)
- "n8n-nodes-base.mysql" (MySQL)

TRAITEMENT:
- "n8n-nodes-base.aggregate" (agr√©gation)
- "n8n-nodes-base.set" (modification donn√©es)
- "n8n-nodes-base.code" (code JavaScript)
- "n8n-nodes-base.markdown" (conversion Markdown)
- "n8n-nodes-base.function" (fonctions)

IA & LANGCHAIN:
- "@n8n/n8n-nodes-langchain.agent" (Agent IA)
- "@n8n/n8n-nodes-langchain.lmChatOpenRouter" (Mod√®le OpenRouter)
- "@n8n/n8n-nodes-langchain.toolCalculator" (Outil Calcul)
- "@n8n/n8n-nodes-langchain.memoryBufferWindow" (M√©moire)

CREDENTIALS - Utilise ces placeholders:
- OpenRouter: {"id": "ADMIN_OPENROUTER_CREDENTIAL_ID", "name": "ADMIN_OPENROUTER_CREDENTIAL_NAME"}
- IMAP: {"id": "USER_IMAP_CREDENTIAL_ID", "name": "USER_IMAP_CREDENTIAL_NAME"}
- SMTP: {"id": "USER_SMTP_CREDENTIAL_ID", "name": "USER_SMTP_CREDENTIAL_NAME"}
- Slack: {"id": "USER_SLACK_CREDENTIAL_ID", "name": "USER_SLACK_CREDENTIAL_NAME"}
- Discord: {"id": "USER_DISCORD_CREDENTIAL_ID", "name": "USER_DISCORD_CREDENTIAL_NAME"}

STRUCTURE OBLIGATOIRE d'un workflow n8n:
{
  "name": "Nom du Workflow",
  "nodes": [
    {
      "id": "node-id-unique",
      "name": "Nom du n≈ìud",
      "type": "type-de-noeud-valide",
      "typeVersion": 1,
      "position": [x, y],
      "parameters": {
        // Param√®tres sp√©cifiques au type de n≈ìud
      },
      "credentials": {
        // Seulement si le n≈ìud n√©cessite des credentials
      },
      "webhookId": "id-webhook" // Seulement pour les webhooks
    }
  ],
  "connections": {
    "Node Name Source": {
      "main": [
        [
          {
            "node": "Node Name Destination",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {},
  "pinData": {},
  "versionId": "1"
}

EXEMPLE R√âSUM√â - Workflow Email avec Agent IA:
- Webhook Trigger ‚Üí IMAP Email (n8n-nodes-base.emailReadImap) ‚Üí Aggregate Emails (destinationFieldName: "data") ‚Üí AI Agent (prompt: "{{ $json.data.toJsonString() }}") ‚Üí Send Email (toEmail: "{{USER_EMAIL}}")
- OpenRouter Chat Model (model: "qwen/qwen2.5-72b-instruct", credentials: ADMIN_OPENROUTER_CREDENTIAL_ID) connect√© via ai_languageModel √† AI Agent
- Calculator Tool et Buffer Window Memory connect√©s via ai_tool et ai_memory √† AI Agent

R√àGLES CRITIQUES - √Ä RESPECTER ABSOLUMENT:
1. Chaque n≈ìud DOIT avoir un "id" unique (format recommand√©: nom-sans-espaces) ET un "name" descriptif
2. Les positions DOIVENT √™tre espac√©es horizontalement (ex: [250, 300], [500, 300], [750, 300])
3. ‚ö†Ô∏è CRITIQUE - Les connexions DOIVENT r√©f√©rencer les NOMS EXACTS des n≈ìuds (pas les IDs) :
   - Dans "connections", utilise le "name" du n≈ìud source comme cl√©
   - Dans chaque connexion, le champ "node" doit contenir le "name" exact du n≈ìud de destination
   - Exemple: Si un n≈ìud a "name": "IMAP Email", utilise "IMAP Email" dans les connexions, pas "imap-email"
4. Les param√®tres DOIVENT correspondre au type de n≈ìud et √™tre r√©alistes
5. Les credentials DOIVENT utiliser les placeholders (USER_XXX_CREDENTIAL_ID)
6. Pour les agents IA, connecte le mod√®le via "ai_languageModel" dans connections (exemple ci-dessous)
7. Les expressions n8n dans les param√®tres utilisent les IDs: {{ $('node-id').item.json.field }} (les IDs sont corrects ici)
8. Les connexions principales utilisent "main" UNIQUEMENT (jamais "next" qui n'existe pas dans n8n)
9. Les connexions IA utilisent "ai_languageModel", "ai_tool", "ai_memory"
10. ‚ö†Ô∏è CRITIQUE - Pour les workflows EMAIL avec IMAP :
    - TOUJOURS utiliser n8n-nodes-base.emailReadImap (PAS n8n-nodes-imap.imap)
    - TOUJOURS ajouter un n≈ìud Aggregate entre IMAP et AI Agent
    - Le n≈ìud Aggregate DOIT avoir "destinationFieldName": "data"
    - Le prompt AI Agent DOIT utiliser {{ $json.data.toJsonString() }} (PAS {{ $json.toJsonString() }})
    - Le champ toEmail SMTP DOIT utiliser {{USER_EMAIL}} ou une adresse hardcod√©e (JAMAIS {{ $('imap-email').item.json.to }})
11. ‚ö†Ô∏è CRITIQUE - Structure workflow obligatoire :
    - TOUJOURS inclure "settings": {} (m√™me si vide) - l'API n8n l'exige
    - TOUJOURS d√©finir "active": false (l'activation se fait apr√®s d√©ploiement via API)
    - TOUJOURS inclure "versionId": "1"

‚ö†Ô∏è R√àGLE OBLIGATOIRE - CONNEXIONS POUR AGENTS IA:
Si tu cr√©es un Agent IA, tu DOIS cr√©er ces 4 n≈ìuds ET les connecter TOUS √† l'agent:

1. "OpenRouter Chat Model" ‚Üí connecte via "ai_languageModel" √† "AI Agent"
2. "Calculator Tool" ‚Üí connecte via "ai_tool" √† "AI Agent"  
3. "Buffer Window Memory" (ou "Simple Memory") ‚Üí connecte via "ai_memory" √† "AI Agent"

Format exact des connexions:
{
  "AI Agent": {
    "main": [[{"node": "Node Suivant", "type": "main", "index": 0}]]
  },
  "OpenRouter Chat Model": {
    "ai_languageModel": [[{"node": "AI Agent", "type": "ai_languageModel", "index": 0}]]
  },
  "Calculator Tool": {
    "ai_tool": [[{"node": "AI Agent", "type": "ai_tool", "index": 0}]]
  },
  "Buffer Window Memory": {
    "ai_memory": [[{"node": "AI Agent", "type": "ai_memory", "index": 0}]]
  }
}

‚ö†Ô∏è IMPORTANT: Les credentials OpenRouter sont ADMIN et sont d√©j√† disponibles - utilise directement ADMIN_OPENROUTER_CREDENTIAL_ID!

Contexte: ${context.templates?.length || 0} templates, n≈ìuds: ${popularNodesList.split(', ').slice(0, 5).join(', ')}

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è INSTRUCTION CRITIQUE - FORMAT DE R√âPONSE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
Tu DOIS r√©pondre UNIQUEMENT avec un objet JSON valide, sans aucun texte avant ou apr√®s.
NE PAS inclure de markdown, de commentaires, d'explications, ou de texte.
COMMENCE directement par { et TERMINE par }.
Exemple de format correct:
{
  "name": "Workflow Name",
  "nodes": [...],
  "connections": {...},
  "settings": {},
  "active": false,
  "versionId": "1"
}

‚ùå FORMAT INCORRECT (NE PAS FAIRE CELA):
"Voici le workflow:"
[bloc markdown json]
{...}
[fin bloc]
"Ce workflow fait..."

‚úÖ FORMAT CORRECT:
{
  "name": "Workflow Name",
  "nodes": [...],
  "connections": {...},
  "settings": {},
  "active": false,
  "versionId": "1"
}

G√©n√®re un workflow COMPLET, FONCTIONNEL et VALIDE. R√©ponds UNIQUEMENT avec le JSON brut, sans texte avant ou apr√®s.`;

      console.log(`üîß [LocalAI] Appel generateContent avec mod√®le: ${model}...`);
      console.log(`üìã [LocalAI] Mod√®les disponibles pour retry: ${availableModelIds.join(', ')}`);
      
      // Essayer de g√©n√©rer avec le mod√®le s√©lectionn√©, si √©chec essayer d'autres mod√®les
      let result;
      let attempts = 0;
      const maxAttempts = Math.max(availableModelIds.length, 3);
      
      while (attempts < maxAttempts) {
        try {
          // Cr√©er un prompt utilisateur d√©taill√© avec instructions sp√©cifiques
          let userMessage = `G√©n√®re un workflow n8n COMPLET et FONCTIONNEL pour: "${description}"\n\n`;
          
          // ‚ö†Ô∏è R√àGLE CRITIQUE - Toujours rappeler l'utilisation des noms dans les connexions
          userMessage += `‚ö†Ô∏è R√àGLE CRITIQUE POUR LES CONNEXIONS:
- Dans la section "connections", utilise TOUJOURS les "name" des n≈ìuds (pas les "id")
- Exemple: Si un n≈ìud a "name": "IMAP Email", utilise "IMAP Email" dans les connexions
- N'utilise JAMAIS "next" dans les connexions - seulement "main", "ai_languageModel", "ai_tool", "ai_memory"

`;
          
          // Ajouter des instructions sp√©cifiques selon la description
          const descLower = description.toLowerCase();
          if (descLower.includes('email') || descLower.includes('imap') || descLower.includes('smtp')) {
            userMessage += `‚ö†Ô∏è R√àGLE CRITIQUE POUR WORKFLOWS EMAIL:

1. IMAP: TOUJOURS utiliser "n8n-nodes-base.emailReadImap" (PAS "n8n-nodes-imap.imap")
   - Param√®tres: { "mailbox": "INBOX", "options": {} }
   - IMAP retourne MULTIPLE items (un par email)

2. AGGREGATE: TOUJOURS ajouter un n≈ìud Aggregate ENTRE IMAP et AI Agent
   - Type: "n8n-nodes-base.aggregate"
   - Param√®tres: { "aggregate": "aggregateAllItemData", "destinationFieldName": "data" }
   - Ce n≈ìud groupe tous les emails dans un champ "data"

3. AI AGENT: Le prompt DOIT utiliser {{ $json.data.toJsonString() }}
   - PAS {{ $json.toJsonString() }} car Aggregate cr√©e le champ "data"
   - Exemple: "Analyse ces emails : {{ $json.data.toJsonString() }}"

4. SMTP toEmail: TOUJOURS utiliser {{USER_EMAIL}} ou une adresse hardcod√©e
   - JAMAIS {{ $('imap-email').item.json.to }} qui peut √™tre vide!

Cha√Æne obligatoire: IMAP Email -> Aggregate Emails -> AI Agent -> Send Email

`;
          }
          
          if (descLower.includes('agent') || descLower.includes('ia') || descLower.includes('ai')) {
            userMessage += `‚ö†Ô∏è R√àGLE OBLIGATOIRE - Si tu cr√©es un Agent IA, tu DOIS automatiquement ajouter:

1. ‚úÖ N≈ìud "@n8n/n8n-nodes-langchain.agent" (nom: "AI Agent")
   - Param√®tres: { "promptType": "define", "text": "..." }

2. ‚úÖ N≈ìud "@n8n/n8n-nodes-langchain.lmChatOpenRouter" (nom: "OpenRouter Chat Model")
   - OBLIGATOIRE: Utilise les credentials ADMIN directement:
     "credentials": {
       "openRouterApi": {
         "id": "ADMIN_OPENROUTER_CREDENTIAL_ID",
         "name": "ADMIN_OPENROUTER_CREDENTIAL_NAME"
       }
     }
   - ‚ö†Ô∏è CRITIQUE: Toujours utiliser le mod√®le le PLUS PERFORMANT mais le MOINS CHER - NE JAMAIS utiliser "anthropic/claude" (trop cher!)
     - Mod√®le recommand√©: "qwen/qwen2.5-72b-instruct" (meilleur ratio performance/prix, fiable et disponible via OpenRouter)
     - Alternative si qwen2.5-72b indisponible: "openai/gpt-4o-mini" (√©conomique et fiable)
     - NE JAMAIS utiliser: "anthropic/claude-3.5-sonnet" ou tout mod√®le anthropic (trop cher!)
     - NE JAMAIS utiliser: "meta-llama/llama-3.1-70b-instruct" (peut ne pas √™tre disponible dans tous les comptes OpenRouter)
   - Param√®tres: { "model": "qwen/qwen2.5-72b-instruct" }
   - Connecte √† l'agent via "ai_languageModel" (utilise les NOMS!)

3. ‚úÖ N≈ìud "@n8n/n8n-nodes-langchain.toolCalculator" (nom: "Calculator Tool")
   - Param√®tres: {} (vide est OK)
   - Connecte √† l'agent via "ai_tool" (utilise les NOMS!)

4. ‚úÖ N≈ìud "@n8n/n8n-nodes-langchain.memoryBufferWindow" (nom: "Buffer Window Memory" ou "Simple Memory")
   - Param√®tres: { "contextWindowLength": 10 } ou { "k": 5 }
   - Connecte √† l'agent via "ai_memory" (utilise les NOMS!)

Ces 4 n≈ìuds sont OBLIGATOIRES si tu cr√©es un Agent IA. Ne les oublie JAMAIS!

`;
          }
          
          // Les r√®gles pour email sont d√©j√† dans la section ci-dessus - ne pas r√©p√©ter
          
          userMessage += `‚ö†Ô∏è RAPPELS FINAUX CRITIQUES:
1. Le JSON g√©n√©r√© DOIT √™tre valide, les n≈ìuds DOIVENT √™tre compatibles n8n, et les connexions DOIVENT √™tre compl√®tes
2. TOUJOURS inclure "settings": {} dans le workflow (m√™me si vide) - l'API n8n l'exige
3. TOUJOURS d√©finir "active": false (l'activation se fait apr√®s d√©ploiement via API)
4. TOUJOURS inclure "versionId": "1"
5. Les connexions DOIVENT utiliser les NOMS des n≈ìuds (pas les IDs)
6. Pour les workflows EMAIL: TOUJOURS ajouter Aggregate entre IMAP et AI Agent`;
          
          // LocalAI utilise le format OpenAI avec system et user messages
          result = await this.generateContent(systemPrompt, model, {
            temperature: 0.1, // Tr√®s d√©terministe pour g√©n√©rer du code JSON fonctionnel
            max_tokens: 4000, // R√©duit pour acc√©l√©rer (workflows g√©n√©ralement < 3000 tokens)
            systemMessage: systemPrompt,
            userMessage: userMessage
          });
          // Si succ√®s, sortir de la boucle
          break;
        } catch (error) {
          attempts++;
          console.warn(`‚ö†Ô∏è [LocalAI] √âchec avec mod√®le ${model} (tentative ${attempts}/${maxAttempts}):`, error.message);
          
          // Si le mod√®le ne peut pas √™tre charg√©, essayer un autre mod√®le disponible
          if (error.message.includes('failed to load model') || error.message.includes('could not load model') || error.message.includes('mkdir') || error.message.includes('no such file')) {
            if (availableModelIds.length > 0 && attempts < maxAttempts) {
              // Essayer le mod√®le suivant dans la liste
              const currentIndex = availableModelIds.findIndex(m => m.toLowerCase() === model.toLowerCase());
              let nextIndex;
              
              if (currentIndex >= 0) {
                // Prendre le mod√®le suivant dans la liste (rotation)
                nextIndex = (currentIndex + 1) % availableModelIds.length;
              } else {
                // Si le mod√®le actuel n'est pas dans la liste, prendre le premier
                nextIndex = 0;
              }
              
              const nextModel = availableModelIds[nextIndex];
              
              if (nextModel && nextModel.toLowerCase() !== model.toLowerCase()) {
                console.log(`üîÑ [LocalAI] Mod√®le ${model} ne peut pas √™tre charg√©, basculement vers: ${nextModel}`);
                model = nextModel;
                attempts--; // Ne pas compter cette tentative comme √©chec, on r√©essaye avec un nouveau mod√®le
                continue; // R√©essayer avec le nouveau mod√®le
              } else {
                console.warn(`‚ö†Ô∏è [LocalAI] Aucun autre mod√®le disponible √† essayer`);
              }
            }
          }
          
          // Si toutes les tentatives ont √©chou√© ou pas de mod√®le de secours, lancer l'erreur
          if (attempts >= maxAttempts) {
            throw new Error(`Impossible de charger un mod√®le fonctionnel sur LocalAI apr√®s ${maxAttempts} tentatives. Derni√®re erreur: ${error.message.substring(0, 200)}`);
          }
          
          // Pour les autres types d'erreurs, relancer
          throw error;
        }
      }
      
      if (!result) {
        throw new Error('√âchec de g√©n√©ration apr√®s toutes les tentatives');
      }

      console.log('üîç [LocalAI] Contenu g√©n√©r√© (premiers 500 caract√®res):', result.content.substring(0, 500));
      console.log('üîç [LocalAI] Contenu g√©n√©r√© (derniers 500 caract√®res):', result.content.substring(Math.max(0, result.content.length - 500)));
      console.log('üîç [LocalAI] Longueur totale:', result.content.length);
      console.log('üîç [LocalAI] Mod√®le utilis√©:', result.model || model);

      // Fonction helper pour extraire et parser le JSON
      const extractAndParseJSON = (content) => {
        // Strat√©gie 1: Chercher du JSON dans un bloc markdown ```json ... ```
        const markdownJsonMatch = content.match(/```json\s*([\s\S]*?)```/i) || 
                                 content.match(/```\s*([\s\S]*?)```/);
        if (markdownJsonMatch) {
          try {
            const jsonStr = markdownJsonMatch[1].trim();
            const parsed = JSON.parse(jsonStr);
            console.log('‚úÖ [LocalAI] JSON extrait depuis bloc markdown');
            return parsed;
          } catch (e) {
            console.warn('‚ö†Ô∏è [LocalAI] √âchec parsing JSON depuis markdown:', e.message);
          }
        }

        // Strat√©gie 2: Chercher le premier bloc JSON valide (du premier { au dernier })
        // Utiliser une approche plus robuste avec comptage des accolades
        let braceCount = 0;
        let startIndex = -1;
        let endIndex = -1;
        
        for (let i = 0; i < content.length; i++) {
          if (content[i] === '{') {
            if (startIndex === -1) {
              startIndex = i;
            }
            braceCount++;
          } else if (content[i] === '}') {
            braceCount--;
            if (braceCount === 0 && startIndex !== -1) {
              endIndex = i;
              break;
            }
          }
        }
        
        if (startIndex !== -1 && endIndex !== -1 && braceCount === 0) {
          try {
            const jsonStr = content.substring(startIndex, endIndex + 1);
            const parsed = JSON.parse(jsonStr);
            console.log('‚úÖ [LocalAI] JSON extrait avec comptage d\'accolades');
            return parsed;
          } catch (e) {
            console.warn('‚ö†Ô∏è [LocalAI] √âchec parsing JSON avec comptage:', e.message);
          }
        }

        // Strat√©gie 3: Regex simple (derni√®re chance)
        const simpleJsonMatch = content.match(/\{[\s\S]*\}/);
        if (simpleJsonMatch) {
          try {
            // Essayer de trouver le JSON complet en cherchant le dernier }
            let jsonStr = simpleJsonMatch[0];
            // Si le JSON est tronqu√©, essayer de le compl√©ter ou de trouver une meilleure correspondance
            let lastBrace = jsonStr.lastIndexOf('}');
            if (lastBrace > 0) {
              jsonStr = jsonStr.substring(0, lastBrace + 1);
            }
            const parsed = JSON.parse(jsonStr);
            console.log('‚úÖ [LocalAI] JSON extrait avec regex simple');
            return parsed;
          } catch (e) {
            console.warn('‚ö†Ô∏è [LocalAI] √âchec parsing JSON avec regex:', e.message);
            // Essayer de nettoyer le JSON (enlever les caract√®res invalides √† la fin)
            try {
              let cleaned = jsonStr;
              // Enlever les caract√®res invalides apr√®s le dernier }
              let lastValidBrace = cleaned.lastIndexOf('}');
              if (lastValidBrace > 0) {
                cleaned = cleaned.substring(0, lastValidBrace + 1);
                // Essayer de trouver un JSON valide en retirant progressivement des caract√®res
                for (let i = cleaned.length - 1; i > 0; i--) {
                  if (cleaned[i] === '}') {
                    try {
                      const testJson = cleaned.substring(0, i + 1);
                      const parsed = JSON.parse(testJson);
                      console.log('‚úÖ [LocalAI] JSON nettoy√© et pars√© avec succ√®s');
                      return parsed;
                    } catch (e2) {
                      // Continuer √† chercher
                    }
                  }
                }
              }
            } catch (e3) {
              console.error('‚ùå [LocalAI] Impossible de nettoyer le JSON');
            }
          }
        }

        // Strat√©gie 4: Essayer de parser directement (au cas o√π c'est d√©j√† du JSON pur)
        try {
          const parsed = JSON.parse(content.trim());
          console.log('‚úÖ [LocalAI] Contenu pars√© directement comme JSON');
          return parsed;
        } catch (e) {
          console.warn('‚ö†Ô∏è [LocalAI] √âchec parsing direct:', e.message);
        }

        return null;
      };

      // Essayer de parser le JSON
      let workflow = extractAndParseJSON(result.content);
      
      if (!workflow) {
        // Afficher plus de d√©tails pour le d√©bogage
        console.error('‚ùå [LocalAI] Impossible d\'extraire le JSON valide');
        console.error('üìù [LocalAI] Contenu complet (premiers 3000 caract√®res):', result.content.substring(0, 3000));
        console.error('üìù [LocalAI] Contenu complet (derniers 1000 caract√®res):', result.content.substring(Math.max(0, result.content.length - 1000)));
        
        // Essayer de trouver des indices sur ce qui ne va pas
        const hasBrace = result.content.includes('{');
        const hasBracket = result.content.includes('}');
        const hasJsonBlock = result.content.includes('```json') || result.content.includes('```');
        const hasTextBefore = !result.content.trim().startsWith('{');
        const hasTextAfter = !result.content.trim().endsWith('}');
        
        console.error('üîç [LocalAI] Analyse:');
        console.error('  - Contient {:', hasBrace);
        console.error('  - Contient }:', hasBracket);
        console.error('  - Contient bloc markdown:', hasJsonBlock);
        console.error('  - Texte avant JSON:', hasTextBefore);
        console.error('  - Texte apr√®s JSON:', hasTextAfter);
        console.error('  - Longueur totale:', result.content.length);
        
        // Essayer de trouver le JSON m√™me s'il y a du texte autour
        if (hasBrace && hasBracket) {
          // Chercher le premier { et le dernier }
          const firstBrace = result.content.indexOf('{');
          const lastBrace = result.content.lastIndexOf('}');
          if (firstBrace >= 0 && lastBrace > firstBrace) {
            const potentialJson = result.content.substring(firstBrace, lastBrace + 1);
            try {
              const parsed = JSON.parse(potentialJson);
              console.log('‚úÖ [LocalAI] JSON trouv√© et extrait apr√®s nettoyage');
              workflow = parsed;
            } catch (e) {
              console.error('‚ùå [LocalAI] JSON extrait mais invalide:', e.message);
              console.error('üìù [LocalAI] JSON extrait (premiers 500 caract√®res):', potentialJson.substring(0, 500));
            }
          }
        }
        
        if (!workflow) {
          if (!hasBrace || !hasBracket) {
            throw new Error(`LocalAI n'a pas g√©n√©r√© de JSON valide. La r√©ponse ne contient pas de structure JSON.\n\nContenu re√ßu (premiers 500 caract√®res):\n${result.content.substring(0, 500)}`);
          }
          
          throw new Error(`LocalAI a g√©n√©r√© du contenu non-JSON valide. Le mod√®le ${model} n'a peut-√™tre pas g√©n√©r√© un JSON valide.\n\nEssayez avec un autre mod√®le ou v√©rifiez que LocalAI r√©pond correctement.\n\nContenu re√ßu (premiers 1000 caract√®res):\n${result.content.substring(0, 1000)}`);
        }
      }
      
      // Validation du workflow g√©n√©r√©
      if (!workflow || typeof workflow !== 'object') {
        throw new Error('Le workflow g√©n√©r√© n\'est pas un objet JSON valide.');
      }
      
      // V√©rifier la structure de base
      if (!workflow.nodes || !Array.isArray(workflow.nodes) || workflow.nodes.length === 0) {
        throw new Error('Le workflow doit contenir au moins un n≈ìud.');
      }
      
      if (!workflow.connections || typeof workflow.connections !== 'object') {
        console.warn('‚ö†Ô∏è [LocalAI] Pas de connexions d√©finies, cr√©ation d\'un objet vide');
        workflow.connections = {};
      }
      
      // V√©rifier que chaque n≈ìud a les champs obligatoires
      for (const node of workflow.nodes) {
        if (!node.id || !node.type || !node.name) {
          console.error('‚ùå [LocalAI] N≈ìud invalide trouv√©:', JSON.stringify(node, null, 2));
          throw new Error(`N≈ìud invalide: chaque n≈ìud doit avoir id, type et name.`);
        }
        
        // V√©rifier que le type de n≈ìud est valide
        const validNodeTypes = [
          'n8n-nodes-base.webhook',
          'n8n-nodes-base.schedule',
          'n8n-nodes-base.manualTrigger',
          'n8n-nodes-base.emailReadImap',
          'n8n-nodes-base.emailSend',
          'n8n-nodes-imap.imap',
          'n8n-nodes-base.slack',
          'n8n-nodes-base.discord',
          'n8n-nodes-base.telegram',
          'n8n-nodes-base.httpRequest',
          'n8n-nodes-base.postgres',
          'n8n-nodes-base.mysql',
          'n8n-nodes-base.aggregate',
          'n8n-nodes-base.set',
          'n8n-nodes-base.code',
          'n8n-nodes-base.markdown',
          'n8n-nodes-base.function',
          '@n8n/n8n-nodes-langchain.agent',
          '@n8n/n8n-nodes-langchain.lmChatOpenRouter',
          '@n8n/n8n-nodes-langchain.toolCalculator',
          '@n8n/n8n-nodes-langchain.memoryBufferWindow'
        ];
        
        // V√©rification plus souple (commence par un type valide)
        const isValidType = validNodeTypes.some(validType => node.type === validType || node.type.startsWith(validType.split('.')[0]));
        
        if (!isValidType && !node.type.includes('langchain') && !node.type.includes('n8n-nodes')) {
          console.warn(`‚ö†Ô∏è [LocalAI] Type de n≈ìud potentiellement invalide: ${node.type}`);
        }
        
        // S'assurer que chaque n≈ìud a une position
        if (!node.position || !Array.isArray(node.position) || node.position.length !== 2) {
          // G√©n√©rer une position automatique
          const index = workflow.nodes.indexOf(node);
          node.position = [250 + (index * 250), 300];
          console.log(`üìç [LocalAI] Position g√©n√©r√©e pour ${node.name}: [${node.position[0]}, ${node.position[1]}]`);
        }
        
        // S'assurer que chaque n≈ìud a typeVersion
        if (!node.typeVersion) {
          node.typeVersion = 1;
        }
        
        // S'assurer que chaque n≈ìud a parameters
        if (!node.parameters) {
          node.parameters = {};
        }
      }
      
      // S'assurer que le workflow a un nom
      if (!workflow.name) {
        workflow.name = `AI Generated Workflow - ${new Date().toISOString().split('T')[0]}`;
      }
      
      // S'assurer que settings existe
      if (!workflow.settings) {
        workflow.settings = {};
      }
      
      // S'assurer que versionId existe
      if (!workflow.versionId) {
        workflow.versionId = "1";
      }
      
      console.log(`‚úÖ [LocalAI] Workflow valid√©: ${workflow.nodes.length} n≈ìuds, ${Object.keys(workflow.connections).length} connexions`);

      return {
        workflow: workflow,
        metadata: {
          model: result.model,
          tokens: result.usage?.total_tokens || 0
        }
      };
    } catch (error) {
      console.error('‚ùå [LocalAI] Erreur lors de la g√©n√©ration du workflow:', error.message);
      console.error('‚ùå [LocalAI] Stack:', error.stack);
      console.error('‚ùå [LocalAI] Mod√®le utilis√©:', model);
      console.error('‚ùå [LocalAI] URL LocalAI:', this.baseUrl);
      throw error;
    }
  }

  // Tester la connexion
  async testConnection() {
    try {
      const isAvailable = await this.isAvailable();
      if (!isAvailable) {
        return { success: false, error: 'LocalAI service non disponible' };
      }

      const models = await this.getAvailableModels();
      return { 
        success: true, 
        models: models.map(m => m.id || m.name),
        message: `LocalAI disponible avec ${models.length} mod√®les`
      };
    } catch (error) {
      return { success: false, error: error.message };
    }
  }
}

module.exports = new OllamaService();

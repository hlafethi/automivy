# ğŸ”§ Configuration Local AI (Ollama) - Conteneur `localai`

## âœ… Configuration mise Ã  jour

**Conteneur Ollama** : `localai`  
**Port** : `19080`  
**VPS** : `147.93.58.155`

## ğŸ¯ Action requise

### 1. CrÃ©er/modifier `backend/.env`

CrÃ©er ou modifier le fichier `backend/.env` et ajouter :

**Si votre backend est AUSSI dans Docker** (mÃªme rÃ©seau que `localai`) :
```bash
OLLAMA_URL=http://localai:19080
```

**Si votre backend n'est PAS dans Docker** :
```bash
OLLAMA_URL=http://147.93.58.155:19080
```

### 2. VÃ©rifier que les conteneurs sont dans le mÃªme rÃ©seau (si backend dans Docker)

```bash
# Sur le VPS
docker network ls
docker network inspect <network_name>

# Si besoin, connecter localai au mÃªme rÃ©seau que le backend
docker network connect <network_name> localai
```

### 3. RedÃ©marrer le backend

```bash
# Si backend dans Docker
docker restart <nom_conteneur_backend>

# Si backend hors Docker
# ArrÃªter (Ctrl+C) et relancer
cd backend
npm start
```

### 4. VÃ©rifier les logs

Dans les logs du backend, vous devriez voir :
```
ğŸ”§ [Ollama] URL configurÃ©e: http://localai:19080
```

### 5. Tester dans l'interface

1. Aller dans **Admin** â†’ **AI Generator**
2. SÃ©lectionner **"Local AI (Ollama - Gratuit)"**
3. Choisir un modÃ¨le (ex: **Llama 3.1 8B**)
4. Entrer une description de workflow
5. Cliquer sur **"Generate with AI"**

## ğŸ” DÃ©pannage

### Erreur : Cannot connect to localai:19080

**Solution** :
- Si backend dans Docker : VÃ©rifier que `localai` et le backend sont dans le mÃªme rÃ©seau
- Si backend hors Docker : Utiliser `OLLAMA_URL=http://147.93.58.155:19080`

### Erreur : Connection refused

**Solution** :
- VÃ©rifier que le conteneur `localai` tourne : `docker ps | grep localai`
- VÃ©rifier que le port est mappÃ© : `docker ps | grep localai` (colonne PORTS)

## ğŸ“ Fichiers modifiÃ©s

- âœ… `env.example` : Ajout de `OLLAMA_URL=http://localai:19080`
- âœ… `backend/services/ollamaService.js` : URL par dÃ©faut mise Ã  `http://localai:19080`
- âœ… `src/components/AIWorkflowGenerator.tsx` : Support Local AI ajoutÃ©

## âœ… Prochaines Ã©tapes

1. âœ… CrÃ©er `backend/.env` avec `OLLAMA_URL=http://localai:19080` (ou IP VPS)
2. âœ… RedÃ©marrer le backend
3. âœ… Tester depuis l'interface Admin â†’ AI Generator

